{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50c43c6-f4a6-48e9-ac6c-1f92ec36d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaTokenizerFast, RobertaConfig, RobertaModelWithHeads\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import AdapterType\n",
    "from sklearn.metrics import f1_score\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8394e391-3494-4a8f-8262-15cb1c98bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee68a40d-1905-4c7a-9685-897f8fe78f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    '''\n",
    "    check gpu status\n",
    "    '''\n",
    "    try:\n",
    "        print('GPU available:', torch.cuda.is_available())\n",
    "        print(torch.cuda.device_count(), 'GPUs detected')\n",
    "        print('Current GPU id:', torch.cuda.current_device())\n",
    "        print('Current GPU Name:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    except:\n",
    "        print('GPU not available')\n",
    "        \n",
    "def encode_batch(batch):\n",
    "    '''\n",
    "    Encodes a batch of input data using the model tokenizer\n",
    "    using 512\n",
    "    '''\n",
    "    return tokenizer(batch[\"text\"], max_length=120, truncation=True, padding=\"max_length\")\n",
    "#     return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'acc': (preds==p.label_ids).mean()}\n",
    "\n",
    "def compute_f1(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'f1': f1_score(p.label_ids, preds, average=f1_type)}\n",
    "\n",
    "def compute_score(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'f1-micro': f1_score(p.label_ids, preds, average='micro'),'f1-macro': f1_score(p.label_ids, preds, average='macro'), 'acc': (preds==p.label_ids).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615a97d6-2cd5-448c-aea3-64cc8bbfecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "1 GPUs detected\n",
      "Current GPU id: 0\n",
      "Current GPU Name: GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb974c5-f4bb-43d6-b8ef-f6fae0d2ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for dataset, name: (classes, type of f1 score)\n",
    "dataset_dict = {'chemprot': (13, 'micro'), 'rct': (5, 'micro'),\n",
    "                'CI': (6, 'macro'), 'sciie': (7, 'm2cro'),\n",
    "                'HN': (2, 'macro'), 'ag': (4, 'macro'),\n",
    "                'amazon': (2, 'macro'), 'imdb': (2, 'macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb5379f-e5ae-425d-9242-8c102249e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'amazon'\n",
    "n_labels = dataset_dict[ds_name][0]\n",
    "f1_type = dataset_dict[ds_name][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef76f85-4293-4193-b9ec-d14fed38c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset task_dataset (C:\\Users\\19197\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(f'data_loaders/{ds_name}_data_loader.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a21622-960e-4d11-afc7-adc642dc82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12f3e9e-aefb-451b-895f-e20cd500c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\19197\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-294b1a221a18a2c0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\19197\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-bea7b60a230a39b6.arrow\n",
      "Loading cached processed dataset at C:\\Users\\19197\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-c40cb088f0753106.arrow\n"
     ]
    }
   ],
   "source": [
    "#dataset_encoded = dataset.map(encode_batch, batched=True, batch_size=512, remove_columns=[\"text\"])\n",
    "dataset_encoded = dataset.map(encode_batch, batched=True)\n",
    "# tokenized_datasets = dataset.map(tokenizer, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c95fce-a352-4d6c-b43b-a1780d4ded49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c8cadc-7004-4369-9e86-8071171cb0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 115251\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89469f46-1f41-4ebe-9610-1fbaf8145fca",
   "metadata": {},
   "source": [
    "## Case 4\n",
    "adapter language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e653ed-da98-4c59-9365-4a8d4caa595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd02982-967f-4861-8143-12aed2e7f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('mlm', AdapterType.text_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45448c4-0f3c-4c39-9885-ee0e7babdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters(['mlm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b8cf123-2f5a-4210-87a5-90aff0c8cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter([\"mlm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22d1bf07-f331-455e-b740-98ac1d75f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 32\n",
    "WARMUP_STEP = max(1,int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11d14126-a368-4b4a-86cb-7541fd125de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/mlm-adapter/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=0.00025,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=max_train_batch_size_mlm,\n",
    "    per_device_eval_batch_size=32,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b341a60e-4d74-4580-838e-8bb37ff2dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cfb61b5-8f80-4dbc-977b-bad162b400a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_mlm = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47071217-f98d-42ca-9618-54846e8a2eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='45000' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45000/45000 24:35:24, Epoch 99/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.589875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.790488</td>\n",
       "      <td>1.562097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.672804</td>\n",
       "      <td>1.556694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.635920</td>\n",
       "      <td>1.526788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.615113</td>\n",
       "      <td>1.495778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.601512</td>\n",
       "      <td>1.496184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.591165</td>\n",
       "      <td>1.481291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.577594</td>\n",
       "      <td>1.494827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.569970</td>\n",
       "      <td>1.469940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.563271</td>\n",
       "      <td>1.497905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.563271</td>\n",
       "      <td>1.465938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.554584</td>\n",
       "      <td>1.473490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.551570</td>\n",
       "      <td>1.460796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.544361</td>\n",
       "      <td>1.456320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.538279</td>\n",
       "      <td>1.450962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.531736</td>\n",
       "      <td>1.450392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.528930</td>\n",
       "      <td>1.444674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.526607</td>\n",
       "      <td>1.451968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.524580</td>\n",
       "      <td>1.448326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.515621</td>\n",
       "      <td>1.452297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.515621</td>\n",
       "      <td>1.436464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.517686</td>\n",
       "      <td>1.450247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.509711</td>\n",
       "      <td>1.432799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.509088</td>\n",
       "      <td>1.443387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.505301</td>\n",
       "      <td>1.429549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.502730</td>\n",
       "      <td>1.428792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.501727</td>\n",
       "      <td>1.435848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.497355</td>\n",
       "      <td>1.443242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.494523</td>\n",
       "      <td>1.452129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.496813</td>\n",
       "      <td>1.420634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.496813</td>\n",
       "      <td>1.410143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.489605</td>\n",
       "      <td>1.424413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.490555</td>\n",
       "      <td>1.440570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.488746</td>\n",
       "      <td>1.423521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.485051</td>\n",
       "      <td>1.414032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.487641</td>\n",
       "      <td>1.422897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.485066</td>\n",
       "      <td>1.404326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.478773</td>\n",
       "      <td>1.416206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.479863</td>\n",
       "      <td>1.410481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.479449</td>\n",
       "      <td>1.421891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.479449</td>\n",
       "      <td>1.416591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.479641</td>\n",
       "      <td>1.428915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.478598</td>\n",
       "      <td>1.425946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.474484</td>\n",
       "      <td>1.399112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.473984</td>\n",
       "      <td>1.426392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.471563</td>\n",
       "      <td>1.408701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.469207</td>\n",
       "      <td>1.423820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.470719</td>\n",
       "      <td>1.401163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.467445</td>\n",
       "      <td>1.417883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.472047</td>\n",
       "      <td>1.396144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.472047</td>\n",
       "      <td>1.408041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.465516</td>\n",
       "      <td>1.427226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.464672</td>\n",
       "      <td>1.416698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.464195</td>\n",
       "      <td>1.409043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.465164</td>\n",
       "      <td>1.385287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.458898</td>\n",
       "      <td>1.388939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.461117</td>\n",
       "      <td>1.411035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.459250</td>\n",
       "      <td>1.406409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.461383</td>\n",
       "      <td>1.406638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.459477</td>\n",
       "      <td>1.406379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.459477</td>\n",
       "      <td>1.399722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.456609</td>\n",
       "      <td>1.378044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.455000</td>\n",
       "      <td>1.422281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.456961</td>\n",
       "      <td>1.408645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.455070</td>\n",
       "      <td>1.393412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.456703</td>\n",
       "      <td>1.396892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.452133</td>\n",
       "      <td>1.395038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.452687</td>\n",
       "      <td>1.410874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.453344</td>\n",
       "      <td>1.404423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.452047</td>\n",
       "      <td>1.404641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.452047</td>\n",
       "      <td>1.413663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.449594</td>\n",
       "      <td>1.415031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.453211</td>\n",
       "      <td>1.405202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.447555</td>\n",
       "      <td>1.385361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.449414</td>\n",
       "      <td>1.402376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.448945</td>\n",
       "      <td>1.385854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.449070</td>\n",
       "      <td>1.399678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.452328</td>\n",
       "      <td>1.389794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.448625</td>\n",
       "      <td>1.386029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.445289</td>\n",
       "      <td>1.410048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.445289</td>\n",
       "      <td>1.392054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.445891</td>\n",
       "      <td>1.401036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.448742</td>\n",
       "      <td>1.398786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.444359</td>\n",
       "      <td>1.393046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.443820</td>\n",
       "      <td>1.378453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.441039</td>\n",
       "      <td>1.392064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.444516</td>\n",
       "      <td>1.395576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.441375</td>\n",
       "      <td>1.396050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.438273</td>\n",
       "      <td>1.407462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.442719</td>\n",
       "      <td>1.402696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.442719</td>\n",
       "      <td>1.402132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.438586</td>\n",
       "      <td>1.388104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.441836</td>\n",
       "      <td>1.402385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.442805</td>\n",
       "      <td>1.398813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.437867</td>\n",
       "      <td>1.392572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.439430</td>\n",
       "      <td>1.386837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.439281</td>\n",
       "      <td>1.408679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.440273</td>\n",
       "      <td>1.390131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.442547</td>\n",
       "      <td>1.399971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.440187</td>\n",
       "      <td>1.371726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45000, training_loss=1.487103125)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07ef6711-b280-43ae-bd5d-cb5306da2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8877d258-27b4-4968-a1ed-8de4651f485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'model/amazon/{today}/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3bbf836-6242-4939-9444-4c580ae61cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(f'model/amazon/{today}/adapter', 'mlm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a568e89-1b70-4ae6-9841-14766f0a4fca",
   "metadata": {},
   "source": [
    "## Baseline Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62de3984-21bf-4445-8218-1153a3c503dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cd5577e-4c89-4ea8-adea-4564c97c231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('Amazon_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e658334-0025-49a0-b646-15fce1b42502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "883ff6e1-73e0-426a-8580-62ebec3791db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics= compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa02a3cd-8522-4709-adb2-8330ab321a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cba4aee-8651-4677-ace2-c72c1c39147a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 48:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.312540</td>\n",
       "      <td>0.868800</td>\n",
       "      <td>0.663893</td>\n",
       "      <td>0.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.339165</td>\n",
       "      <td>0.305352</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.643489</td>\n",
       "      <td>0.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.294193</td>\n",
       "      <td>0.310035</td>\n",
       "      <td>0.874600</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.874600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1350, training_loss=0.30662575050636576)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "544072e5-97c7-42a7-9de5-1911ba7d23d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='470' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3100349009037018,\n",
       " 'eval_f1-micro': 0.8746,\n",
       " 'eval_f1-macro': 0.6655568702139828,\n",
       " 'eval_acc': 0.8746,\n",
       " 'epoch': 2.999444752915047}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1579c4a-77b6-4c23-aa9f-cae2ba39c623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3098597824573517,\n",
       " 'eval_f1-micro': 0.87452,\n",
       " 'eval_f1-macro': 0.6588806506844302,\n",
       " 'eval_acc': 0.87452,\n",
       " 'epoch': 2.999444752915047}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc30eb2-9222-4d05-ad30-9dd92f9c3987",
   "metadata": {},
   "source": [
    "## Pretrained Baseline TAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a101a0bb-8016-4419-8d9d-02386ae4108e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cd34921a8f4ed89f31b95f4b76af5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=430.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7132e131004e358b80f6972f2b8a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=655615582.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/dsp_roberta_base_tapt_amazon_helpfulness_115K were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at allenai/dsp_roberta_base_tapt_amazon_helpfulness_115K and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('allenai/dsp_roberta_base_tapt_amazon_helpfulness_115K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efa4ff86-4443-47e2-b67e-f50a87b9d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('Amazon_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8996397-a137-4e79-95be-f84549b3afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac92d3a7-3b20-4a46-aad3-3926adb72580",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics= compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "499e8532-d4e4-414f-b797-03ea99ee79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/pt-ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76439717-a74e-404a-8e3c-520948600c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 48:58, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.297437</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>0.683938</td>\n",
       "      <td>0.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321902</td>\n",
       "      <td>0.296483</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.695489</td>\n",
       "      <td>0.881800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.272898</td>\n",
       "      <td>0.298727</td>\n",
       "      <td>0.879800</td>\n",
       "      <td>0.697147</td>\n",
       "      <td>0.879800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1350, training_loss=0.28658112702546296)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d82205f6-69a6-4035-9d75-b2dfeb2b4edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='470' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.29872748255729675,\n",
       " 'eval_f1-micro': 0.8798,\n",
       " 'eval_f1-macro': 0.6971471666227758,\n",
       " 'eval_acc': 0.8798,\n",
       " 'epoch': 2.999444752915047}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7430701d-0de1-4756-a554-8ebaa86c3a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.29575395584106445,\n",
       " 'eval_f1-micro': 0.8817200000000001,\n",
       " 'eval_f1-macro': 0.693303043249042,\n",
       " 'eval_acc': 0.88172,\n",
       " 'epoch': 2.999444752915047}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8deaa8-b332-4988-b086-6d7c8419a1fe",
   "metadata": {},
   "source": [
    "## Adapter Pretrain Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "854d10da-1322-4fc2-82e8-8a940c3cfa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b3a665-4fb7-4465-a015-bc9a1c10feda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlm'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_adapter('model/amazon/20210429/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dbbfe77-7b66-4cb1-be18-89f9453aaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters(['mlm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd3626ce-38d5-4cb8-9954-7045e961e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('Amazon_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70c0d88c-dd33-4d6e-8186-7ae107f59b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "# GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "# print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97dbc15c-e4c7-4e39-a1a1-9468829f26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/adapter-pt-ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c671df03-a2e8-46dc-acde-899a8de71746",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics=compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca3d3c-f584-4442-ae8d-fa6ffe696c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3903' max='21612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3903/21612 13:24 < 1:00:51, 4.85 it/s, Epoch 0.54/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc276aee-7fc9-44ea-9f8b-509fd261d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c216e3f-01c2-40c5-acab-cadecc337c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6988f4-e6f3-4023-91bd-51ef359fba7e",
   "metadata": {},
   "source": [
    "## Adapter Pretrain Adapter Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a102f8ee-27be-4e3c-88ca-b57714b0ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e012680b-c122-4b3b-b820-adc4262687d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlm'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_adapter('model/amazon/20210429/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a314226-b31e-4aa4-8cfe-7cdbba3acc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('Amazon_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e75d9a9-860a-4744-bd10-ac79f75ee614",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('Amazon_classifier', adapter_type=AdapterType.text_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1529b64d-e8bc-4c17-ace9-53f0b34898ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters([['mlm', 'Amazon_classifier']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5905cf99-12bf-467d-888c-08885f36bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter(['Amazon_classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9559da0d-376e-42a1-9859-fd7815e86c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "# GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "# print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ac8ee9d-eea7-47c6-b917-535a827735bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/adapter-pt-adapter-ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08c12cc0-5294-4dfa-a90a-914391125395",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics= compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f83705ac-fe29-4e77-a8dd-96dd8d06ba90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 01:17, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.805332</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>0.043860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.782240</td>\n",
       "      <td>0.149123</td>\n",
       "      <td>0.064966</td>\n",
       "      <td>0.149123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.773764</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.114341</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=1.9204243554009333)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "763a15b2-aaa2-4347-9bf8-8e8b18c5c674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='37' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 02:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7737641334533691,\n",
       " 'eval_f1-micro': 0.5175438596491229,\n",
       " 'eval_f1-macro': 0.11434108527131782,\n",
       " 'eval_acc': 0.5175438596491229,\n",
       " 'epoch': 2.9056603773584904}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aca42f7e-81e6-4fd1-ab9d-7df3836f72c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7735462188720703,\n",
       " 'eval_f1-micro': 0.5107913669064749,\n",
       " 'eval_f1-macro': 0.1126984126984127,\n",
       " 'eval_acc': 0.5107913669064749,\n",
       " 'epoch': 2.9056603773584904}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50c43c6-f4a6-48e9-ac6c-1f92ec36d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaTokenizerFast, RobertaConfig, RobertaModelWithHeads\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import AdapterType\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab4eb81-fb06-436f-8a2a-e2a0536488bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8394e391-3494-4a8f-8262-15cb1c98bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee68a40d-1905-4c7a-9685-897f8fe78f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    '''\n",
    "    check gpu status\n",
    "    '''\n",
    "    try:\n",
    "        print('GPU available:', torch.cuda.is_available())\n",
    "        print(torch.cuda.device_count(), 'GPUs detected')\n",
    "        print('Current GPU id:', torch.cuda.current_device())\n",
    "        print('Current GPU Name:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    except:\n",
    "        print('GPU not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d83ef4-aa9d-41e3-89e8-6ed4378e3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(batch):\n",
    "    '''\n",
    "    Encodes a batch of input data using the model tokenizer\n",
    "    using 512\n",
    "    '''\n",
    "    return tokenizer(batch[\"text\"], max_length=120, truncation=True, padding=\"max_length\")\n",
    "#     return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615a97d6-2cd5-448c-aea3-64cc8bbfecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "1 GPUs detected\n",
      "Current GPU id: 0\n",
      "Current GPU Name: GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb974c5-f4bb-43d6-b8ef-f6fae0d2ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for dataset, name: (classes, type of f1 score)\n",
    "dataset_dict = {'chemprot': (13, 'micro'), 'rct': (5, 'micro'),\n",
    "                'CI': (6, 'macro'), 'sciie': (7, 'm2cro'),\n",
    "                'HN': (2, 'macro'), 'ag': (4, 'macro'),\n",
    "                'amazon': (2, 'macro'), 'imdb': (2, 'macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb5379f-e5ae-425d-9242-8c102249e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'amazon'\n",
    "n_labels = dataset_dict[ds_name][0]\n",
    "f1_type = dataset_dict[ds_name][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef76f85-4293-4193-b9ec-d14fed38c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset task_dataset (C:\\Users\\19197\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(f'data_loaders/{ds_name}_data_loader.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a21622-960e-4d11-afc7-adc642dc82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12f3e9e-aefb-451b-895f-e20cd500c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\19197\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-294b1a221a18a2c0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\19197\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-bea7b60a230a39b6.arrow\n",
      "Loading cached processed dataset at C:\\Users\\19197\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-c40cb088f0753106.arrow\n"
     ]
    }
   ],
   "source": [
    "#dataset_encoded = dataset.map(encode_batch, batched=True, batch_size=512, remove_columns=[\"text\"])\n",
    "dataset_encoded = dataset.map(encode_batch, batched=True)\n",
    "# tokenized_datasets = dataset.map(tokenizer, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c95fce-a352-4d6c-b43b-a1780d4ded49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33c8cadc-7004-4369-9e86-8071171cb0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 115251\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89469f46-1f41-4ebe-9610-1fbaf8145fca",
   "metadata": {},
   "source": [
    "## Case 4\n",
    "adapter language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e653ed-da98-4c59-9365-4a8d4caa595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd02982-967f-4861-8143-12aed2e7f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('mlm', AdapterType.text_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45448c4-0f3c-4c39-9885-ee0e7babdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters(['mlm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b8cf123-2f5a-4210-87a5-90aff0c8cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter([\"mlm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22d1bf07-f331-455e-b740-98ac1d75f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 32\n",
    "WARMUP_STEP = max(1,int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11d14126-a368-4b4a-86cb-7541fd125de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/mlm-adapter/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=0.00025,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=max_train_batch_size_mlm,\n",
    "    per_device_eval_batch_size=32,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b341a60e-4d74-4580-838e-8bb37ff2dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cfb61b5-8f80-4dbc-977b-bad162b400a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_mlm = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47071217-f98d-42ca-9618-54846e8a2eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='45000' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45000/45000 24:35:24, Epoch 99/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.589875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.790488</td>\n",
       "      <td>1.562097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.672804</td>\n",
       "      <td>1.556694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.635920</td>\n",
       "      <td>1.526788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.615113</td>\n",
       "      <td>1.495778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.601512</td>\n",
       "      <td>1.496184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.591165</td>\n",
       "      <td>1.481291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.577594</td>\n",
       "      <td>1.494827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.569970</td>\n",
       "      <td>1.469940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.563271</td>\n",
       "      <td>1.497905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.563271</td>\n",
       "      <td>1.465938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.554584</td>\n",
       "      <td>1.473490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.551570</td>\n",
       "      <td>1.460796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.544361</td>\n",
       "      <td>1.456320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.538279</td>\n",
       "      <td>1.450962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.531736</td>\n",
       "      <td>1.450392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.528930</td>\n",
       "      <td>1.444674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.526607</td>\n",
       "      <td>1.451968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.524580</td>\n",
       "      <td>1.448326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.515621</td>\n",
       "      <td>1.452297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.515621</td>\n",
       "      <td>1.436464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.517686</td>\n",
       "      <td>1.450247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.509711</td>\n",
       "      <td>1.432799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.509088</td>\n",
       "      <td>1.443387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.505301</td>\n",
       "      <td>1.429549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.502730</td>\n",
       "      <td>1.428792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.501727</td>\n",
       "      <td>1.435848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.497355</td>\n",
       "      <td>1.443242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.494523</td>\n",
       "      <td>1.452129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.496813</td>\n",
       "      <td>1.420634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.496813</td>\n",
       "      <td>1.410143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.489605</td>\n",
       "      <td>1.424413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.490555</td>\n",
       "      <td>1.440570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.488746</td>\n",
       "      <td>1.423521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.485051</td>\n",
       "      <td>1.414032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.487641</td>\n",
       "      <td>1.422897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.485066</td>\n",
       "      <td>1.404326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.478773</td>\n",
       "      <td>1.416206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.479863</td>\n",
       "      <td>1.410481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.479449</td>\n",
       "      <td>1.421891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.479449</td>\n",
       "      <td>1.416591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.479641</td>\n",
       "      <td>1.428915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.478598</td>\n",
       "      <td>1.425946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.474484</td>\n",
       "      <td>1.399112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.473984</td>\n",
       "      <td>1.426392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.471563</td>\n",
       "      <td>1.408701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.469207</td>\n",
       "      <td>1.423820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.470719</td>\n",
       "      <td>1.401163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.467445</td>\n",
       "      <td>1.417883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.472047</td>\n",
       "      <td>1.396144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.472047</td>\n",
       "      <td>1.408041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.465516</td>\n",
       "      <td>1.427226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.464672</td>\n",
       "      <td>1.416698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.464195</td>\n",
       "      <td>1.409043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.465164</td>\n",
       "      <td>1.385287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.458898</td>\n",
       "      <td>1.388939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.461117</td>\n",
       "      <td>1.411035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.459250</td>\n",
       "      <td>1.406409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.461383</td>\n",
       "      <td>1.406638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.459477</td>\n",
       "      <td>1.406379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.459477</td>\n",
       "      <td>1.399722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.456609</td>\n",
       "      <td>1.378044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.455000</td>\n",
       "      <td>1.422281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.456961</td>\n",
       "      <td>1.408645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.455070</td>\n",
       "      <td>1.393412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.456703</td>\n",
       "      <td>1.396892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.452133</td>\n",
       "      <td>1.395038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.452687</td>\n",
       "      <td>1.410874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.453344</td>\n",
       "      <td>1.404423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.452047</td>\n",
       "      <td>1.404641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.452047</td>\n",
       "      <td>1.413663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.449594</td>\n",
       "      <td>1.415031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.453211</td>\n",
       "      <td>1.405202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.447555</td>\n",
       "      <td>1.385361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.449414</td>\n",
       "      <td>1.402376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.448945</td>\n",
       "      <td>1.385854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.449070</td>\n",
       "      <td>1.399678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.452328</td>\n",
       "      <td>1.389794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.448625</td>\n",
       "      <td>1.386029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.445289</td>\n",
       "      <td>1.410048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.445289</td>\n",
       "      <td>1.392054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.445891</td>\n",
       "      <td>1.401036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.448742</td>\n",
       "      <td>1.398786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.444359</td>\n",
       "      <td>1.393046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.443820</td>\n",
       "      <td>1.378453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.441039</td>\n",
       "      <td>1.392064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.444516</td>\n",
       "      <td>1.395576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.441375</td>\n",
       "      <td>1.396050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.438273</td>\n",
       "      <td>1.407462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.442719</td>\n",
       "      <td>1.402696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.442719</td>\n",
       "      <td>1.402132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.438586</td>\n",
       "      <td>1.388104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.441836</td>\n",
       "      <td>1.402385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.442805</td>\n",
       "      <td>1.398813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.437867</td>\n",
       "      <td>1.392572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.439430</td>\n",
       "      <td>1.386837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.439281</td>\n",
       "      <td>1.408679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.440273</td>\n",
       "      <td>1.390131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.442547</td>\n",
       "      <td>1.399971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.440187</td>\n",
       "      <td>1.371726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45000, training_loss=1.487103125)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3196cee2-f380-4d82-b74d-5f2c88fd203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4022356271743774, 'epoch': 99.99944475291505}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_mlm.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07ef6711-b280-43ae-bd5d-cb5306da2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8877d258-27b4-4968-a1ed-8de4651f485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'model/amazon/{today}/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3bbf836-6242-4939-9444-4c580ae61cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(f'model/amazon/{today}/adapter', 'mlm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

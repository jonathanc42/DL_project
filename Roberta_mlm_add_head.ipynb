{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50c43c6-f4a6-48e9-ac6c-1f92ec36d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaTokenizerFast, RobertaConfig, RobertaModelWithHeads\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import AdapterType\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab4eb81-fb06-436f-8a2a-e2a0536488bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8394e391-3494-4a8f-8262-15cb1c98bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee68a40d-1905-4c7a-9685-897f8fe78f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    '''\n",
    "    check gpu status\n",
    "    '''\n",
    "    try:\n",
    "        print('GPU available:', torch.cuda.is_available())\n",
    "        print(torch.cuda.device_count(), 'GPUs detected')\n",
    "        print('Current GPU id:', torch.cuda.current_device())\n",
    "        print('Current GPU Name:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    except:\n",
    "        print('GPU not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d83ef4-aa9d-41e3-89e8-6ed4378e3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(batch):\n",
    "    '''\n",
    "    Encodes a batch of input data using the model tokenizer\n",
    "    '''\n",
    "    return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
    "#     return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615a97d6-2cd5-448c-aea3-64cc8bbfecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "1 GPUs detected\n",
      "Current GPU id: 0\n",
      "Current GPU Name: NVIDIA GeForce GTX 980 Ti\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb974c5-f4bb-43d6-b8ef-f6fae0d2ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for dataset, name: (classes, type of f1 score)\n",
    "dataset_dict = {'chemprot': (13, 'micro'), 'rct': (5, 'micro'),\n",
    "                'CI': (6, 'macro'), 'sciie': (7, 'm2cro'),\n",
    "                'HN': (2, 'macro'), 'ag': (4, 'macro'),\n",
    "                'amazon': (2, 'macro'), 'imdb': (2, 'macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb5379f-e5ae-425d-9242-8c102249e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'amazon'\n",
    "n_labels = dataset_dict[ds_name][0]\n",
    "f1_type = dataset_dict[ds_name][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef76f85-4293-4193-b9ec-d14fed38c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset task_dataset (C:\\Users\\snow-\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(f'data_loaders/{ds_name}_data_loader.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a21622-960e-4d11-afc7-adc642dc82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12f3e9e-aefb-451b-895f-e20cd500c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\snow-\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-a7bd2fe3fa422dc1.arrow\n",
      "Loading cached processed dataset at C:\\Users\\snow-\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-1ed36f07475b7f6a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\snow-\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\d4dbb1ae1e5b21302597f18c62e58ab7f320999e2bdffea6d0514c3c329ad9ae\\cache-a74e264e5d1d56f8.arrow\n"
     ]
    }
   ],
   "source": [
    "#dataset_encoded = dataset.map(encode_batch, batched=True, batch_size=512, remove_columns=[\"text\"])\n",
    "dataset_encoded = dataset.map(encode_batch, batched=True)\n",
    "# tokenized_datasets = dataset.map(tokenizer, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c95fce-a352-4d6c-b43b-a1780d4ded49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d1187ef-72f5-4aff-b6b2-23df1cb57490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/amazon_mlm.pt were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at model/amazon_mlm.pt and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model = RobertaModelWithHeads.from_pretrained('model/amazon_mlm.pt', return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd13ee-9abd-48b0-afdd-47eeec1d8e10",
   "metadata": {},
   "source": [
    "## Case 2\n",
    "fine tune a sequence classification model with a task corpus pretrained mlm model by adding a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50f5dedf-eb9a-46c8-a9ff-b3043b190a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add_adapter(\"classifier\", adapter_type=AdapterType.text_task, config=\"pfeiffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73466d84-15cc-4920-950f-c0ad2578aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train_adapter(['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b9b9857-e524-4689-aa9d-a06cf8d267e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head(\"classifier\", num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6692275d-fc43-4aeb-95b4-31c4c8f5bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_active_adapters([[\"classifier\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "701cc772-bf1b-4f28-b772-7b32eed01ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model output\n",
    "# model(**tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\"))\n",
    "# test model output\n",
    "# model(**tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f1aafc1-a20c-46ba-b079-86a63cb8a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add_adapter('language', AdapterType.text_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e031d5c9-9244-4ef0-9f62-9a588d22ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train_adapter('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a894f73-08a5-4a0d-b03e-e92371188ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bebd0204-9909-4435-96d8-96cba613a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/ft-mlm/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "044fcfbd-ee57-4788-b746-7522c3e182c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0405e3b-fe6c-46a7-8453-c4c6d35293dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir='model/amazon/ft-mlm/20210427/', overwrite_output_dir=False, do_train=False, do_eval=None, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=64, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2, max_steps=-1, warmup_steps=0, logging_dir='runs\\\\Apr27_10-43-12_DESKTOP-HAMNFK0', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='model/amazon/ft-mlm/20210427/', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a1d501a-3daf-4f46-b52c-5b652ee5e893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='7204' max='7204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7204/7204 1:07:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.309073</td>\n",
       "      <td>0.311008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271893</td>\n",
       "      <td>0.319173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7204, training_loss=0.30229963930628817)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ee866dc-e8eb-4a63-b5b6-662206143c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.31917300820350647, 'epoch': 2.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "# {'eval_loss': 0.31917300820350647, 'epoch': 2.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6be76d-8cbe-40ef-97a1-21fe561f3527",
   "metadata": {},
   "source": [
    "## Case 3\n",
    "Compare with raw RoBerta sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f51739-eb88-4cc7-b482-f56507cadb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_class = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da5ea5a-ef75-49d5-8e3e-a602713ecbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/ft-raw/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b46787-2d60-43e8-ae1c-69046dc346cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_class = Trainer(\n",
    "    model=model_class,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a34226c4-cfba-49e5-93b7-8be11ce99dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='7204' max='7204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7204/7204 1:06:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.317709</td>\n",
       "      <td>0.316337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.280312</td>\n",
       "      <td>0.321011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7204, training_loss=0.3124534695893774)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_class.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df4e4669-6b50-4155-b9e2-49a0bc94e8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.32101142406463623, 'epoch': 2.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_class.evaluate()\n",
    "# {'eval_loss': 0.32101142406463623, 'epoch': 2.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239aa93-fc42-44a9-851c-6efe70162483",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_dspt = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/ft-mlm/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_ratio=0.006,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836cab9f-1aa8-4df3-bf89-e4ea3732427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperparameter Assignment\n",
    "number of epochs 3 or 10\n",
    "patience 3\n",
    "batch size 16\n",
    "learning rate 2e-5\n",
    "dropout 0.1\n",
    "feedforward layer 1\n",
    "feedforward nonlinearity tanh\n",
    "classification layer 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89469f46-1f41-4ebe-9610-1fbaf8145fca",
   "metadata": {},
   "source": [
    "## Case 4\n",
    "adapter language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5bb5c8-809a-4503-8fed-a7e4ec730f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = RobertaModelWithHeads.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e653ed-da98-4c59-9365-4a8d4caa595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd02982-967f-4861-8143-12aed2e7f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('mlm', AdapterType.text_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b8cf123-2f5a-4210-87a5-90aff0c8cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter([\"mlm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45448c4-0f3c-4c39-9885-ee0e7babdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters(['mlm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7ae4a0d-797e-4767-ab69-c3cc20d3ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add_classification_head(\"classifier\", num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1670a90-25b6-46d6-883a-c614babf98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add_adapter('classifier_adapter', AdapterType.text_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d00f93a-bb7d-4a1e-b27a-4f93a3d5806d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.6095625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio=0.006\n",
    "train_batch_size=32\n",
    "dataset_encoded['train'].num_rows / train_batch_size * warmup_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a878b751-9533-40d7-99a3-83df784a15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/mlm-adapter/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=0.00025,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=21,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "500fa796-fa6f-43c1-8ff6-9b24227876f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02e7ddfd-242c-48b9-9d65-d2d56f15fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47071217-f98d-42ca-9618-54846e8a2eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='52' max='7204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  52/7204 00:23 < 56:22, 2.11 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

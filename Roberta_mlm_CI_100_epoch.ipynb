{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50c43c6-f4a6-48e9-ac6c-1f92ec36d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaTokenizerFast, RobertaConfig, RobertaModelWithHeads\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import AdapterType\n",
    "from sklearn.metrics import f1_score\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8394e391-3494-4a8f-8262-15cb1c98bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee68a40d-1905-4c7a-9685-897f8fe78f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    '''\n",
    "    check gpu status\n",
    "    '''\n",
    "    try:\n",
    "        print('GPU available:', torch.cuda.is_available())\n",
    "        print(torch.cuda.device_count(), 'GPUs detected')\n",
    "        print('Current GPU id:', torch.cuda.current_device())\n",
    "        print('Current GPU Name:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    except:\n",
    "        print('GPU not available')\n",
    "        \n",
    "def encode_batch(batch):\n",
    "    '''\n",
    "    Encodes a batch of input data using the model tokenizer\n",
    "    using 512\n",
    "    '''\n",
    "    return tokenizer(batch[\"text\"], max_length=120, truncation=True, padding=\"max_length\")\n",
    "#     return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'acc': (preds==p.label_ids).mean()}\n",
    "\n",
    "def compute_f1(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'f1': f1_score(p.label_ids, preds, average=f1_type)}\n",
    "\n",
    "def compute_score(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'f1-micro': f1_score(p.label_ids, preds, average='micro'),'f1-macro': f1_score(p.label_ids, preds, average='macro'), 'acc': (preds==p.label_ids).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615a97d6-2cd5-448c-aea3-64cc8bbfecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "1 GPUs detected\n",
      "Current GPU id: 0\n",
      "Current GPU Name: NVIDIA GeForce GTX 980 Ti\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb974c5-f4bb-43d6-b8ef-f6fae0d2ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for dataset, name: (classes, type of f1 score)\n",
    "dataset_dict = {'chemprot': (13, 'micro'), 'rct': (5, 'micro'),\n",
    "                'CI': (6, 'macro'), 'sciie': (7, 'm2cro'),\n",
    "                'HN': (2, 'macro'), 'ag': (4, 'macro'),\n",
    "                'amazon': (2, 'macro'), 'imdb': (2, 'macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb5379f-e5ae-425d-9242-8c102249e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'CI'\n",
    "n_labels = dataset_dict[ds_name][0]\n",
    "f1_type = dataset_dict[ds_name][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef76f85-4293-4193-b9ec-d14fed38c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset task_dataset (C:\\Users\\snow-\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\0f3e47e2404d89192a80ec5140374817ceee56a5e8ced3e23f680be3c7719815)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(f'data_loaders/{ds_name}_data_loader.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a21622-960e-4d11-afc7-adc642dc82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12f3e9e-aefb-451b-895f-e20cd500c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\snow-\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\0f3e47e2404d89192a80ec5140374817ceee56a5e8ced3e23f680be3c7719815\\cache-2a3cfd2cdaee0e91.arrow\n",
      "Loading cached processed dataset at C:\\Users\\snow-\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\0f3e47e2404d89192a80ec5140374817ceee56a5e8ced3e23f680be3c7719815\\cache-b19c49c1112d7830.arrow\n",
      "Loading cached processed dataset at C:\\Users\\snow-\\.cache\\huggingface\\datasets\\task_dataset\\task\\1.0.0\\0f3e47e2404d89192a80ec5140374817ceee56a5e8ced3e23f680be3c7719815\\cache-e00caab88a68deb3.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_encoded = dataset.map(encode_batch, batched=True)\n",
    "# tokenized_datasets = dataset.map(tokenizer, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c95fce-a352-4d6c-b43b-a1780d4ded49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c8cadc-7004-4369-9e86-8071171cb0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 1688\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 114\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89469f46-1f41-4ebe-9610-1fbaf8145fca",
   "metadata": {},
   "source": [
    "## Train adapter language model 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e653ed-da98-4c59-9365-4a8d4caa595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd02982-967f-4861-8143-12aed2e7f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('mlm', AdapterType.text_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45448c4-0f3c-4c39-9885-ee0e7babdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters(['mlm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b8cf123-2f5a-4210-87a5-90aff0c8cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter([\"mlm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22d1bf07-f331-455e-b740-98ac1d75f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio)\n",
    "print(WARMUP_STEP)\n",
    "GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11d14126-a368-4b4a-86cb-7541fd125de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/mlm-adapter/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=0.00025,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=max_train_batch_size_mlm,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b341a60e-4d74-4580-838e-8bb37ff2dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cfb61b5-8f80-4dbc-977b-bad162b400a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_mlm = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47071217-f98d-42ca-9618-54846e8a2eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 1:10:59, Epoch 99/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.565787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.320081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.409102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.174234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.011321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.944114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.399259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.890951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.010864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.973657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.141691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.081038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.169699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.037678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.907337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.798821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.794194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.873639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.022371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.939868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.972272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.932429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.913461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.876657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.775292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.019374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.929437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.873995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.986934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.741186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.743544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.798485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.782161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.839154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.831077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.784491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.699034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.660488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.957401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.817023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.846128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.977208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.928782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.818051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.755935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.804297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.749168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.906822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.692723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.665882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.771887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.912544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.730098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.823734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.754045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.732664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.864484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.882430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.837741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.739146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.791346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.763960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.593912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.811093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.651864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.680983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.763848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.687707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.791601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.603167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.847363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.787184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.592880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.711309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.889728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.616014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.587585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.610079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.621905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.769906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.727666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.602610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.658942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.926299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.767772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.577881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.661611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.709210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.921640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.802030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.937111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.593390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.627317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.761988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.706843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.715275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.866614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.666921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.074518</td>\n",
       "      <td>1.919391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=2.024574788411458)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3196cee2-f380-4d82-b74d-5f2c88fd203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.698409914970398, 'epoch': 99.90566037735849}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_mlm.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07ef6711-b280-43ae-bd5d-cb5306da2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8877d258-27b4-4968-a1ed-8de4651f485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'model/CI/{today}/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3bbf836-6242-4939-9444-4c580ae61cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(f'model/CI/{today}/adapter', 'mlm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85907aab-cfb4-4d5d-a7e8-656a3ece8c62",
   "metadata": {},
   "source": [
    "## Baseline Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ad361d-7723-4cfd-8c4c-5799014cb578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5a9a81a-a8d5-461b-a76b-d6479653604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('CI_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49dc7345-d519-4bae-8ff1-03cdff89661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "# GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "# print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db1860c0-c263-49eb-a3e4-446138ae6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "980bbe60-af92-4096-b079-4f8b24327efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics= compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdefd900-ae21-478f-852e-fe994e063d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 02:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.068134</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.243859</td>\n",
       "      <td>0.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.869458</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.368697</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.856226</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.363781</td>\n",
       "      <td>0.719298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=318, training_loss=1.055605522491647)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fde2544-0763-4b0a-8bed-76e9d1e41867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8562257885932922,\n",
       " 'eval_f1-micro': 0.7192982456140351,\n",
       " 'eval_f1-macro': 0.3637812848716722,\n",
       " 'eval_acc': 0.7192982456140351,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd27e6f8-f638-45c3-9cd7-c5a9cfab30ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8781317472457886,\n",
       " 'eval_f1-micro': 0.7194244604316546,\n",
       " 'eval_f1-macro': 0.41375367551838144,\n",
       " 'eval_acc': 0.7194244604316546,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e65015-be71-439c-b1b4-1548ad397bf3",
   "metadata": {},
   "source": [
    "## TAPT Pretrained baseline finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a4e972-faa8-4174-90c8-55f44772dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/dsp_roberta_base_tapt_citation_intent_1688 were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at allenai/dsp_roberta_base_tapt_citation_intent_1688 and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('allenai/dsp_roberta_base_tapt_citation_intent_1688')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8be59811-4237-4fbf-a418-3d8bec3a300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('CI_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46ba61f8-5e8f-4824-b821-dd488917c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "# GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "# print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12eb8167-d0df-4adc-8ade-02b13696e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/tapt-ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f384efc-7316-40be-9724-fe5bf9d59247",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics=compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476c3104-6355-4fd1-a8eb-643dc31564dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 02:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.989810</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.322286</td>\n",
       "      <td>0.692982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.804230</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.352298</td>\n",
       "      <td>0.692982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.780640</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.413875</td>\n",
       "      <td>0.692982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=318, training_loss=0.9623676036138954)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd07a331-aa9d-4b96-a81f-c42ceb74dba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7806396484375,\n",
       " 'eval_f1-micro': 0.6929824561403509,\n",
       " 'eval_f1-macro': 0.41387488328664795,\n",
       " 'eval_acc': 0.6929824561403509,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e9f184a-0d7e-4e80-905a-32136102a6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.835127592086792,\n",
       " 'eval_f1-micro': 0.7553956834532374,\n",
       " 'eval_f1-macro': 0.4780421932739813,\n",
       " 'eval_acc': 0.7553956834532374,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b26f00-10d7-4b26-8cef-c4acfda3f133",
   "metadata": {},
   "source": [
    "## Adapter Pretrain Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1d5b4a2-e264-45f1-aa56-2dd379a62222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1d88a83-41ce-4dbe-b8b4-ff5e3d2aa2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlm'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_adapter('model/CI/20210428/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1df340f-fd2b-4635-bc91-a1b734317e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('CI_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07ae04cd-e437-42a3-8aa2-dacf7c27d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters([\"mlm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "774d0116-5947-4000-9373-b524c3fb8a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "# GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "# print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb1cf9d5-a40a-40d4-8260-1303d0f751f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/adapter-pt-ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41ae2509-bf1a-494c-9de0-c6e790a9d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics=compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7015a93-1d99-4bb0-9dae-c70392d26bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 02:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.062377</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.243859</td>\n",
       "      <td>0.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.913780</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883724</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=318, training_loss=1.0902853911777712)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a89c9c-b6cb-44d8-86aa-005f21e62c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8837238550186157,\n",
       " 'eval_f1-micro': 0.7105263157894737,\n",
       " 'eval_f1-macro': 0.3614690320572674,\n",
       " 'eval_acc': 0.7105263157894737,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c3edf4a-a2cc-4406-b484-f25fadeeea26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9179366827011108,\n",
       " 'eval_f1-micro': 0.6906474820143885,\n",
       " 'eval_f1-macro': 0.34601846902089034,\n",
       " 'eval_acc': 0.6906474820143885,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a309cb-9221-4db8-8482-e847957f3933",
   "metadata": {},
   "source": [
    "## Adapter Pretrain Adapter Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3427eb-5231-4f3b-a2a1-1f4e530e6101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ec81458-bd0b-4f6d-8a69-e0923e114779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlm'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_adapter('model/CI/20210428/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7661cad6-88af-4267-bd12-6c4e8c865130",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('CI_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07728bb2-3494-4f58-bbf3-25d8e9aaed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(\"CI_classifier\", adapter_type=AdapterType.text_task, config=\"pfeiffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a1c2da7-7c10-4e2c-8447-fdecc329bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters([['mlm', 'CI_classifier']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e63cb52-b4eb-48d2-a2f6-c1d8fcbea77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter(['CI_classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a6ca68-bef0-400a-970e-95e7b311a3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "# GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "# print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fce0ae0d-acab-43cd-bc2e-58e9b4ca4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/adapter-pt-adapter-ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5de0fb9-6576-423b-b5ed-e4a4a622ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics= compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7bc5147-e8da-491f-b36c-ae5d1ae90c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 01:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.408942</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.356578</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.346379</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=318, training_loss=1.4399384312659689)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5816d51b-629d-4b6f-aa18-f822cee00b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.346379041671753,\n",
       " 'eval_f1-micro': 0.5175438596491229,\n",
       " 'eval_f1-macro': 0.11368015414258188,\n",
       " 'eval_acc': 0.5175438596491229,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e7474f6-287f-4c21-8728-455b4f6472bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3637382984161377,\n",
       " 'eval_f1-micro': 0.5107913669064749,\n",
       " 'eval_f1-macro': 0.1126984126984127,\n",
       " 'eval_acc': 0.5107913669064749,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1ef5e-0d0c-476e-bf26-6d085f608f5b",
   "metadata": {},
   "source": [
    "## Adapter Pretrain Adapter Finetune more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189e28c8-de06-4717-81b7-d120def3eede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4e8731-c7f6-42ce-a5d0-8f34e2fcb232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlm'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_adapter('model/CI/20210428/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23cd5e40-8821-4061-a4c9-1ad4b24adf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head('CI_classifier', num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7277a5e6-36ad-4279-a346-0d46f0beaaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(\"CI_classifier\", adapter_type=AdapterType.text_task, config=\"pfeiffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b6f63e0-566e-4ee6-93dd-68948dd7b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters([['mlm', 'CI_classifier']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d6f4ef1-663e-4590-ad5b-73b230232f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_adapter(['CI_classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81922beb-ae2b-42dc-a66b-0cf7a8c293c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get warm up steps for given warmup ratio\n",
    "\n",
    "warmup_ratio = 0.006\n",
    "max_train_batch_size_mlm = 16\n",
    "WARMUP_STEP = max(1, int(dataset_encoded['train'].num_rows / max_train_batch_size_mlm * warmup_ratio))\n",
    "print(WARMUP_STEP)\n",
    "# GRADIENT_ACC_STEP = 256 / max_train_batch_size_mlm\n",
    "# print(GRADIENT_ACC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d486df7-0607-4d8b-9671-ba16605efe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=f'model/{ds_name}/adapter-pt-adapter-ft/{today}/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=40,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "#     warmup_ratio=0.006, not supported in adapter-transformers\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    weight_decay=0.01,\n",
    "#     gradient_accumulation_steps=GRADIENT_ACC_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3496897-1395-417a-b356-310b37c4f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"validation\"],\n",
    "    compute_metrics= compute_score\n",
    "#     data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea4533e-acbb-463e-b32f-ce57bd05443e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='4240' max='4240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4240/4240 22:06, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.375445</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.322572</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.307114</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.285271</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>1.262777</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>1.230609</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.517544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>1.186542</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.140460</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>1.112144</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.214447</td>\n",
       "      <td>0.587719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>1.063328</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>0.274325</td>\n",
       "      <td>0.622807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.198104</td>\n",
       "      <td>1.023273</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.283257</td>\n",
       "      <td>0.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.198104</td>\n",
       "      <td>1.011970</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.344236</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.198104</td>\n",
       "      <td>0.970287</td>\n",
       "      <td>0.675439</td>\n",
       "      <td>0.326477</td>\n",
       "      <td>0.675439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.198104</td>\n",
       "      <td>0.962033</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.322977</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.198104</td>\n",
       "      <td>0.922622</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.325988</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.032384</td>\n",
       "      <td>0.913355</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.334719</td>\n",
       "      <td>0.692982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.032384</td>\n",
       "      <td>0.897512</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.334646</td>\n",
       "      <td>0.692982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.032384</td>\n",
       "      <td>0.885356</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.334819</td>\n",
       "      <td>0.692982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.032384</td>\n",
       "      <td>0.874017</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.332088</td>\n",
       "      <td>0.692982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.947029</td>\n",
       "      <td>0.855904</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.335029</td>\n",
       "      <td>0.692982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.947029</td>\n",
       "      <td>0.867858</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.352667</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.947029</td>\n",
       "      <td>0.853944</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.701754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.947029</td>\n",
       "      <td>0.828943</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.328480</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.947029</td>\n",
       "      <td>0.827250</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.352605</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.887423</td>\n",
       "      <td>0.821289</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.331101</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.887423</td>\n",
       "      <td>0.810570</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.329502</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.887423</td>\n",
       "      <td>0.812211</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.347564</td>\n",
       "      <td>0.701754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.887423</td>\n",
       "      <td>0.827648</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.365280</td>\n",
       "      <td>0.728070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.887423</td>\n",
       "      <td>0.809077</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.366697</td>\n",
       "      <td>0.728070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.834317</td>\n",
       "      <td>0.814545</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.366697</td>\n",
       "      <td>0.728070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.834317</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.360023</td>\n",
       "      <td>0.719298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.834317</td>\n",
       "      <td>0.802943</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.360023</td>\n",
       "      <td>0.719298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.834317</td>\n",
       "      <td>0.800314</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.435763</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.834317</td>\n",
       "      <td>0.804978</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.435728</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.813399</td>\n",
       "      <td>0.795002</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.429013</td>\n",
       "      <td>0.728070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.813399</td>\n",
       "      <td>0.793592</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.435763</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.813399</td>\n",
       "      <td>0.792216</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.429013</td>\n",
       "      <td>0.728070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.813399</td>\n",
       "      <td>0.789908</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.429013</td>\n",
       "      <td>0.728070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.790937</td>\n",
       "      <td>0.793841</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.435763</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.790937</td>\n",
       "      <td>0.790622</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.429013</td>\n",
       "      <td>0.728070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.790937</td>\n",
       "      <td>0.790860</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.435763</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4240, training_loss=0.973677610001474)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80933fe2-832c-4014-8bf2-c97f425b8540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7908603549003601,\n",
       " 'eval_f1-micro': 0.7368421052631579,\n",
       " 'eval_f1-macro': 0.435762679055362,\n",
       " 'eval_acc': 0.7368421052631579,\n",
       " 'epoch': 40.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3051f81-e262-4c36-b16e-dfce1149b8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9183534979820251,\n",
       " 'eval_f1-micro': 0.7050359712230215,\n",
       " 'eval_f1-macro': 0.43262295348761515,\n",
       " 'eval_acc': 0.7050359712230215,\n",
       " 'epoch': 40.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ft.evaluate(dataset_encoded['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5cc0f-1c86-41c1-a3f1-b47dc1b1c111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

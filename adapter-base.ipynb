{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supposed-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig, RobertaModelWithHeads\n",
    "from transformers import AdapterType\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handmade-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for dataset, name: (classes, type of f1 score)\n",
    "dataset_dict = {'chemprot': (13, 'micro'), 'rct': (5, 'micro'),\n",
    "                'CI': (6, 'macro'), 'sciie': (7, 'm2cro'),\n",
    "                'HN': (2, 'macro'), 'ag': (4, 'macro'),\n",
    "                'amazon': (2, 'macro'), 'imdb': (2, 'macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerical-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'chemprot'\n",
    "n_labels = dataset_dict[ds_name][0]\n",
    "f1_type = dataset_dict[ds_name][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "novel-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset task_dataset (/home/qiyuan/.cache/huggingface/datasets/task_dataset/task/1.0.0/5154b9e4ddb61d69749d3fc1f0afa65b89f7dac4c88e7d21a6d00c5090e813b4)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(f'data_loaders/{ds_name}_data_loader.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greenhouse-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModelWithHeads.from_pretrained('roberta-base')\n",
    "model.add_adapter(ds_name, AdapterType.text_task)\n",
    "model.train_adapter([ds_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tribal-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_classification_head(ds_name, num_labels=n_labels)\n",
    "model.set_active_adapters([[ds_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "framed-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(batch):\n",
    "    return tokenizer(batch['text'], max_length=120, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "absent-coral",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/qiyuan/.cache/huggingface/datasets/task_dataset/task/1.0.0/5154b9e4ddb61d69749d3fc1f0afa65b89f7dac4c88e7d21a6d00c5090e813b4/cache-6ef7a533742e09c3.arrow\n",
      "Loading cached processed dataset at /home/qiyuan/.cache/huggingface/datasets/task_dataset/task/1.0.0/5154b9e4ddb61d69749d3fc1f0afa65b89f7dac4c88e7d21a6d00c5090e813b4/cache-30c8391a4d964213.arrow\n",
      "Loading cached processed dataset at /home/qiyuan/.cache/huggingface/datasets/task_dataset/task/1.0.0/5154b9e4ddb61d69749d3fc1f0afa65b89f7dac4c88e7d21a6d00c5090e813b4/cache-32485901e8dc5002.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(encode_batch, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "classical-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'acc': (preds==p.label_ids).mean()}\n",
    "\n",
    "def compute_f1(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'f1': f1_score(p.label_ids, preds, average=f1_type)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moral-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_a = TrainingArguments(output_dir='./results',\n",
    "                        num_train_epochs=9,\n",
    "                        per_device_train_batch_size=16,\n",
    "                        per_device_eval_batch_size=16,\n",
    "                        remove_unused_columns=False)\n",
    "trainer = Trainer(model=model,\n",
    "                  args=t_a,\n",
    "                  train_dataset=dataset['train'],\n",
    "                  eval_dataset=dataset['validation'],\n",
    "                  compute_metrics=compute_f1\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "seven-proof",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2349' max='2349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2349/2349 05:42, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.576207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.866955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.657101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.598273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2349, training_loss=0.8707611616645381)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "latest-morocco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6644195318222046, 'eval_f1': 0.8034610630407911, 'epoch': 9.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "driven-housing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/dsp_roberta_base_tapt_chemprot_4169 were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at allenai/dsp_roberta_base_tapt_chemprot_4169 and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tapt_model = RobertaModelWithHeads.from_pretrained('allenai/dsp_roberta_base_tapt_chemprot_4169')\n",
    "tapt_model.add_adapter(ds_name, AdapterType.text_task)\n",
    "tapt_model.train_adapter([ds_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "confused-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "tapt_model.add_classification_head(ds_name, num_labels=n_labels)\n",
    "tapt_model.set_active_adapters([[ds_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "built-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "tapt_trainer = Trainer(model=tapt_model,\n",
    "                  args=t_a,\n",
    "                  train_dataset=dataset['train'],\n",
    "                  eval_dataset=dataset['validation'],\n",
    "                  compute_metrics=compute_f1\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "finite-forward",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2349' max='2349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2349/2349 05:47, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.545737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.866464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.679146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.604712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2349, training_loss=0.8714210896757929)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tapt_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "varying-sculpture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7092375755310059, 'eval_f1': 0.7824474660074165, 'epoch': 9.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tapt_trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
